{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make_imc_data was changed and sparsify_with_mask was added\n",
    "from sgimc.utils import make_imc_data, sparsify, sparsify_with_mask\n",
    "\n",
    "from sgimc import IMCProblem\n",
    "\n",
    "from sgimc.qa_objective import QAObjectiveL2Loss\n",
    "from sgimc.qa_objective import QAObjectiveLogLoss\n",
    "from sgimc.qa_objective import QAObjectiveHuberLoss\n",
    "\n",
    "from sgimc.algorithm.admm import sub_0_cg\n",
    "from sgimc.algorithm.admm import sub_0_lbfgs\n",
    "from sgimc.algorithm.admm import sub_m\n",
    "\n",
    "from sgimc import imc_descent\n",
    "from sgimc.utils import performance\n",
    "\n",
    "from sgimc.utils import plot_WH, plot_loss\n",
    "\n",
    "# loss calculation and boolean mask invertion\n",
    "from utils import calculate_loss, invert\n",
    "\n",
    "PROBLEM = \"classification\" if False else \"regression\"\n",
    "random_state = np.random.RandomState(0x0BADCAFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sgimc.algorithm import admm_step\n",
    "from sgimc.algorithm.decoupled import step as decoupled_step\n",
    "\n",
    "def step_qaadmm(problem, W, H, C, eta, method=\"l-bfgs\", sparse=True,\n",
    "                n_iterations=50, rtol=1e-5, atol=1e-8):\n",
    "\n",
    "    approx_type = \"quadratic\" if method in (\"cg\",) else \"linear\"\n",
    "    Obj = problem.objective(W, H, approx_type=approx_type)\n",
    "\n",
    "    return admm_step(Obj, W, C, eta, sparse=sparse, method=method,\n",
    "                     n_iterations=n_iterations, rtol=rtol, atol=atol)\n",
    "\n",
    "def step_decoupled(problem, W, H, C, eta, rtol=1e-5, atol=1e-8):\n",
    "\n",
    "    Obj = problem.objective(W, H, approx_type=\"linear\")\n",
    "\n",
    "    return decoupled_step(Obj, W, C, eta, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_fn = step_qaadmm\n",
    "\n",
    "if PROBLEM == \"classification\":\n",
    "    QAObjectiveLoss = QAObjectiveLogLoss\n",
    "else:\n",
    "    QAObjectiveLoss = QAObjectiveL2Loss  # QAObjectiveHuberLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROBLEM == \"classification\":\n",
    "    C = 1e0, 1e-1, 1e-3\n",
    "    eta = 1e0\n",
    "else:\n",
    "    # C = 2e-5, 2e-3, 0\n",
    "    C = 2e-3, 2e-4, 1e-4\n",
    "    eta = 1e1\n",
    "    \n",
    "step_kwargs = {\n",
    "    \"C\": C,                 # the regularizr constants (C_lasso, C_group, C_ridge)\n",
    "    \"eta\": eta,             # the eta of the ADMM (larger - faster but more unstable)\n",
    "    \"rtol\": 1e-5,           # the relative tolerance for stopping the ADMM\n",
    "    \"atol\": 1e-8,           # the absolute tolerance\n",
    "    \"method\": \"cg\",         # the method to use in Sub_0\n",
    "    \"n_iterations\": 2,      # the number of iterations of the inner ADMM\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters for matrix sizes\n",
    "n_samples, n_objects = 811, 915\n",
    "n_rank = 26\n",
    "n_features = 100\n",
    "K = 25\n",
    "\n",
    "# magnitude parameters\n",
    "scale = 0.05\n",
    "noise_to_signal = 0.15\n",
    "\n",
    "# number of iterations\n",
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 15.95it/s]\n"
     ]
    }
   ],
   "source": [
    "X, W_ideal, Y, H_ideal, R_noisy_full, R_clear_full = make_imc_data(\n",
    "    n_samples, n_features, n_objects, n_features,\n",
    "    n_rank, scale=(0.05, 0.05), noise=0.05*0.15,\n",
    "    binarize=(PROBLEM == \"classification\"),\n",
    "    random_state=random_state,\n",
    "    return_noisy_only=False                                         # new parameter\n",
    ")\n",
    "\n",
    "# we use noisy data for training procedure\n",
    "R_train, mask = sparsify(R_noisy_full, 0.10, random_state=random_state) \n",
    "problem = IMCProblem(QAObjectiveLoss, X, Y, R_train, n_threads=8)\n",
    "\n",
    "\n",
    "W_0 = random_state.normal(size=(X.shape[1], K))\n",
    "H_0 = random_state.normal(size=(Y.shape[1], K))\n",
    "\n",
    "W, H = W_0.copy(), H_0.copy()\n",
    "\n",
    "W, H = imc_descent(problem, W, H,\n",
    "                   step_fn,                  # the inner optimization\n",
    "                   step_kwargs=step_kwargs,  # asrtguments for the inner optimizer\n",
    "                   n_iterations=n_iter,      # the number of outer iterations (Gauss-Siedel)\n",
    "                   return_history=True,      # Record the evolution of the matrices (W, H)\n",
    "                   rtol=1e-5,                # relative stopping tolerance for the outer iterations\n",
    "                   atol=1e-7,                # absolute tolerance\n",
    "                   verbose=True,             # show the progress bar\n",
    "                   check_product=True,       # use the product W H' for stopping\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on noisy test: 0.288921020322\n",
      "Loss on clear test: 0.0343073527558\n",
      "Loss on noisy train: 0.271307777496\n",
      "Loss on clear train: 0.032023181309\n"
     ]
    }
   ],
   "source": [
    "mask_test = invert(mask)\n",
    "\n",
    "print('Loss on noisy test:', calculate_loss(R_noisy_full, X, W, H, Y, mask_test))\n",
    "print('Loss on clear test:', calculate_loss(R_clear_full, X, W, H, Y, mask_test))\n",
    "print('Loss on noisy train:', calculate_loss(R_noisy_full, X, W, H, Y, mask))\n",
    "print('Loss on clear train:', calculate_loss(R_clear_full, X, W, H, Y, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
