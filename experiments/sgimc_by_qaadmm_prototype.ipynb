{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse-Group Lasso Inductive Matrix Completion via ADMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext cython\n",
    "\n",
    "class DelayedKeyboardInterrupt(object):\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.getsignal(signal.SIGINT)\n",
    "        signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBLEM = \"classification\" if False else \"regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(0x0BADCAFE)\n",
    "\n",
    "b_add_noise = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a low rank matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_objects = 19990, 201 # 199, 201\n",
    "n_rank, n_features = 5, 20  # 15, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_objects = 1990, 2010\n",
    "n_rank, n_features = 5, 20  # 15, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_objects = 199, 2010\n",
    "n_rank, n_features = 5, 20  # 15, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_objects = 199, 201\n",
    "n_rank, n_features = 5, 200  # 15, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_objects = 1990, 2010\n",
    "n_rank, n_features = 5, 100  # 15, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_objects = 550, 550\n",
    "n_rank, n_features = 5, 25  # 15, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_objects = 75550, 40\n",
    "n_rank, n_features = 5, 20  # 15, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_objects = 199, 201\n",
    "n_rank, n_features = 5, 20  # 15, 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert PROBLEM in (\"classification\", \"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgimc.utils import make_imc_data, sparsify\n",
    "\n",
    "X, W_ideal, Y, H_ideal, R_full = make_imc_data(\n",
    "    n_samples, n_features, n_objects, n_features,\n",
    "    n_rank, scale=(0.05, 0.05), noise=0,\n",
    "    binarize=PROBLEM == \"classification\",\n",
    "    random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the feature coefficients look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgimc.util import plot_WH, plot_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $P\\Sigma Q' = X$ is the full SVD of $X$, and $X$ is full-rank ($m$),\n",
    "then the induced projector onto $X$'s columns space is:\n",
    "$$ X(X'X)^{-1}X\n",
    "    = P\\Sigma Q'(Q\\Sigma'P'P\\Sigma Q')^{-1}Q\\Sigma' P'\n",
    "    = P\\Sigma Q'Q (\\Sigma'\\Sigma)^{-1} Q'Q\\Sigma' P'\n",
    "    = P\\Sigma (\\Sigma'\\Sigma)^{-1} \\Sigma' P'\n",
    "    = \\sum_{i=1}^m P e_i e_i' P'\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assert that $\\mathtt{col}(R) \\subseteq \\mathtt{col}(X)$ and $\\mathtt{row}(R) \\subseteq \\mathtt{col}(Y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PX, s, QX = np.linalg.svd(X, full_matrices=1)\n",
    "PY, s, QY = np.linalg.svd(Y, full_matrices=1)\n",
    "\n",
    "proj_X = np.dot(PX[:, :n_features], PX[:, :n_features].T)\n",
    "proj_Y = np.dot(PY[:, :n_features], PY[:, :n_features].T)\n",
    "\n",
    "if not b_add_noise and PROBLEM != \"classification\":\n",
    "    assert np.allclose(np.dot(proj_X, R_full), R_full)\n",
    "    assert np.allclose(np.dot(proj_Y, R_full.T), R_full.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the bulk of the values from $R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, mask = sparsify(R_full, 0.10, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, title=\"The synthetic matrix\")\n",
    "ax.imshow(R.todense(), cmap=plt.cm.RdBu, origin=\"upper\")\n",
    "\n",
    "print(\"Observed entries: %d / %d\" % (R.nnz, np.prod(R.shape)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The IMC problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMC problem is:\n",
    "$$\\begin{aligned}\n",
    "    & \\underset{W, H}{\\text{miminize}}\n",
    "      & & \\sum_{(i,j)\\in \\Omega} l(p_{ij}, R_{ij})\n",
    "          + \\nu_W \\sum_{m=1}^{d_1} \\bigl\\| W' e_m \\bigr\\|_2\n",
    "          + \\nu_H \\sum_{m=1}^{d_2} \\bigl\\| H' e_m \\bigr\\|_2\n",
    "          + \\mu_W \\bigl\\| W \\bigr\\|_1\n",
    "          + \\mu_H \\bigl\\| H \\bigr\\|_1\n",
    "            \\,, \\\\\n",
    "    & \\text{with}\n",
    "      & & p_{ij} = e_i'\\, X W \\, H' Y'\\, e_j\n",
    "      \\,,\n",
    "\\end{aligned}$$\n",
    "where $X \\in \\mathbb{R}^{n_1 \\times d_1}$, $Y \\in \\mathbb{R}^{n_2 \\times d_2}$,\n",
    "$W \\in \\mathbb{R}^{d_1\\times k}$ and $H \\in \\mathbb{R}^{d_2\\times k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Approximation\n",
    "The target objective without regularization (holding $H$ fixed) is\n",
    "$$ F(W; H)\n",
    "    = \\sum_{(i,j)\\in \\Omega}\n",
    "        l(p_{ij}, R_{ij})\n",
    "    \\,, $$\n",
    "in which $p = p(W) = (e_i' X W H' Y' e_j)_{(i,j)\\in \\Omega}$ are the current\n",
    "predictions.\n",
    "\n",
    "The Quadratic Approximation to $F$ around $W_0$ is\n",
    "$$ Q(W; W_0)\n",
    "    = F(W_0)\n",
    "        + \\nabla F(W_0)' \\delta\n",
    "        + \\frac12 \\delta' \\nabla^2 F(W_0) \\delta\n",
    "    \\,, $$\n",
    "for $\\delta = \\mathtt{vec}(W - W_0)$. Now the gradient of $F$ w.r.t. vec-form of $W$ is\n",
    "$$ \\nabla F(W_0)\n",
    "    = \\mathtt{vec}\\bigl(\n",
    "        X' g Y H\n",
    "    \\bigr)\n",
    "    \\,, $$\n",
    "with $g = g(W_0) = (l{'}_{p}(p(W_0)_{ij}, R_{ij}))_{(i,j)\\in \\Omega}$ is $\\Omega$-sparse\n",
    "matrix of first-order (gradient) data. For a matrix $D \\in \\mathbb{R}^{d_1 \\times k}$\n",
    "$$ \\nabla^2F(W_0)\\, \\mathtt{vec}(D)\n",
    "    = \\mathtt{vec}\\Bigl(\n",
    "        X' \\underbrace{\\bigl\\{h \\odot (X D H'Y')\\bigr\\}}_{\\Omega-\\text{sparse}} YH\n",
    "    \\Bigr)\n",
    "    \\,, $$\n",
    "where $h = h(W_0) = (l{''}_{pp}(p(W_0)_{ij}, R_{ij}))_{(i,j)\\in \\Omega}$ is\n",
    "the $\\Omega$-sparse matrix of the second order (hessian) values and $\\odot$\n",
    "is the element-wise matrix product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic approximation with respect to $H$ around $H_0$ holding $W$ and $\\Sigma$ fixed\n",
    "is similar up to transposing $R$ and swapping $X \\leftrightarrow Y$ and $W \\leftrightarrow H$\n",
    "in the above formulae."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that although the expressions for the gradient and the hessian-vector product presented above are identical to the fast operations in *section 3.1* of [H. Yu et al. (2014)](http://bigdata.ices.utexas.edu/publication/993/), the fomulae here have been derived independently. In fact, they are obvious products of simple block-matrix and **vech** algebra.\n",
    "\n",
    "The implementation below is, however, completely original (although, nothing special)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the gradient and the hessian-vector product we need the following\n",
    "\"elementary\" operations:\n",
    "* $\\mathtt{Op}_d: D \\mapsto (e_i' X D H'Y' e_j)_{(i, j)\\in \\Omega}$ -- a map\n",
    "of some $\\mathbb{R}^{d_1\\times k}$ dense $D$ to a $\\mathbb{R}^{n_1\\times n_2}$\n",
    "$\\Omega$-sparse matrix $S$;\n",
    "* $\\mathtt{Op}_s: S \\mapsto X'S YH$ mapping an $\\mathbb{R}^{n_1\\times n_2}$\n",
    "$\\Omega$-sparse $S$ to a $\\mathbb{R}^{d_1\\times k}$ dense matrix $D$.\n",
    "\n",
    "The gradient becomes\n",
    "$$ \\nabla F(W_0)\n",
    "    = \\mathtt{vec}(\\mathtt{Op}_s(g)) \\,, $$\n",
    "and the hessian-vector product transforms into\n",
    "$$ \\nabla^2 F(W_0)\\,\\mathtt{vec}(D)\n",
    "    = \\mathtt{vec}\\bigl(\\mathtt{Op}_s\\bigl(h\\odot \\mathtt{Op}_d(D)\\bigr)\\bigr) \\,. $$\n",
    "\n",
    "In fact the predictions $p = p(W_0)$ also form an $\\mathbb{R}^{n_1\\times n_2}$\n",
    "$\\Omega$-matrix, that cam be computed by $p(W_0) = \\mathtt{Op}_d(W_0)$. The\n",
    "gradient $g(W_0)$ and hessian $h(W_0)$ statistics are also $\\Omega$-sparse,\n",
    "and can be computed by element-wise application of $l'_p$ and $l''_{pp}$\n",
    "to $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar formulae hold for $H$ with appropriate re-labellings and transpositions.\n",
    "\n",
    "\n",
    "**Note** that the sparsity structure remains unchanged and the thin matrix $YH$\n",
    "can be cached, since both $H$ and $Y$ fit in memory and $k < d_2 \\ll n_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sgimc import op_s, op_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $l_2$ loss $l(p, t) = \\frac12 (p-t)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgimc.qa_objective import QAObjectiveL2Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-loss $l(p, t) = \\log \\bigl(1 + e^{-t p}\\bigr)$ for $t\\in \\{-1, +1\\}$\n",
    "and $p\\in \\mathbb{R}$.\n",
    "\n",
    "\\begin{align}\n",
    "    \\sigma(x)\n",
    "        &= \\frac1{1+e^{-x}}\n",
    "        \\,, \\\\\n",
    "    \\sigma'(x)\n",
    "        &= -\\frac{- e^{-x}}{(1+e^{-x})^2}\n",
    "        = \\frac{e^{-x}}{1+e^{-x}} \\frac1{1+e^{-x}}\n",
    "        = (1-\\sigma(x))\\,\\sigma(x)\n",
    "        \\,, \\\\\n",
    "    l(p, t)\n",
    "        &= \\log \\bigl(1 + e^{-t p}\\bigr)\n",
    "         = \\log \\bigl(1 + e^{- \\lvert p \\rvert}\\bigr)\n",
    "         - \\min\\bigl\\{t p, 0\\bigr\\}\n",
    "        \\,, \\\\\n",
    "    l_p'(p, t)\n",
    "        &= \\frac{-t e^{-t p}}{1 + e^{-t p}}\n",
    "         = -t (1 - \\sigma(t p))\n",
    "        \\,, \\\\\n",
    "      l_p''(p, t)\n",
    "        &= (1 - \\sigma(t p))\\sigma(t p)\n",
    "        = (1 - \\sigma(p))\\sigma(p)\n",
    "        \\,.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgimc.qa_objective import QAObjectiveLogLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber loss:\n",
    "$$ l(x; \\epsilon)\n",
    "    = \\begin{cases}\n",
    "        \\frac12 x^2\n",
    "            & \\text{if } \\lvert x \\rvert \\leq \\epsilon\\,, \\\\\n",
    "        \\epsilon \\bigl(\\lvert x \\rvert - \\frac\\epsilon2\\bigr)\n",
    "            & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    \\,. $$\n",
    "Therefore\n",
    "$$ l_p'\n",
    "    = \\begin{cases}\n",
    "        x   & \\text{if } \\lvert x \\rvert \\leq \\epsilon\\,, \\\\\n",
    "        \\epsilon \\frac{x}{\\lvert x \\rvert}\n",
    "            & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    \\,, $$\n",
    "and\n",
    "$$ l_{pp}''\n",
    "    = \\begin{cases}\n",
    "        1   & \\text{if } \\lvert x \\rvert \\leq \\epsilon\\,, \\\\\n",
    "        0   & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgimc.qa_objective import QAObjectiveHuberLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROBLEM == \"classification\":\n",
    "    QAObjectiveLoss = QAObjectiveLogLoss\n",
    "else:\n",
    "    QAObjectiveLoss = QAObjectiveL2Loss  # QAObjectiveHuberLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix $H$ and consider the problem with respect to $W$:\n",
    "$$\\begin{aligned}\n",
    "    & \\underset{W \\in \\mathbb{R}^{d_1\\times k}}{\\text{miminize}}\n",
    "      & & Q(W; W_0)\n",
    "          + \\sum_{m=1}^{d_1}\n",
    "              \\nu_m \\bigl\\| W' e_m \\bigr\\|_2\n",
    "              + \\mu_m \\bigl\\| W' e_m \\bigr\\|_1\n",
    "              + \\frac{\\kappa_m}2 \\bigl\\| W' e_m \\bigr\\|_2^2\n",
    "            \\,.\n",
    "\\end{aligned}$$\n",
    "Let's move to an equivalent problem by splitting the variables in\n",
    "the objective, introducing linear consensus constraints and adding\n",
    "$d_1$ ridge-like regularizers (augmenation)\n",
    "$$\\begin{aligned}\n",
    "    & \\underset{Z_m, \\delta_m \\in \\mathbb{R}^{k\\times 1}}{\\text{miminize}}\n",
    "      & & Q(\\delta; W_0)\n",
    "          + \\sum_{m=1}^{d_1}\n",
    "              \\nu_m \\bigl\\| Z_m \\bigr\\|_2\n",
    "              + \\mu_m \\bigl\\| Z_m \\bigr\\|_1\n",
    "              + \\frac{\\kappa_m}2 \\bigl\\| Z_m \\bigr\\|_2^2\n",
    "          + \\frac1{2\\eta}\n",
    "              \\sum_{m=1}^{d_1} \\bigl\\| \\delta_m - (Z_m - W_0'e_m) \\bigr\\|_2^2\n",
    "          \\,, \\\\\n",
    "    & \\text{subject to}\n",
    "      & & Z_m - \\delta_m = W_0' e_m\\,, m=1 \\ldots d_1\n",
    "          \\,,\n",
    "\\end{aligned}$$\n",
    "with $\\sum_{m=1}^{d_1} e_m \\delta_m' = \\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is convex and the constraints are linear, which means that\n",
    "Strong Duality holds for this problem. The lagrangian is\n",
    "\\begin{align}\n",
    "    \\mathcal{L}(Z_m, \\delta_m; \\lambda_m)\n",
    "        &= F(W_0)\n",
    "            + \\nabla F(W_0)' \\mathtt{vec}(\\delta)\n",
    "            + \\frac12 \\mathtt{vec}(\\delta)' \\nabla^2 F(W_0) \\mathtt{vec}(\\delta)\n",
    "        \\\\\n",
    "        & + \\sum_{m=1}^{d_1}\n",
    "              \\nu_m \\bigl\\| Z_m \\bigr\\|_2\n",
    "               + \\mu_m \\bigl\\| Z_m \\bigr\\|_1\n",
    "               + \\frac{\\kappa_m}2 \\bigl\\| Z_m \\bigr\\|_2^2\n",
    "        \\\\\n",
    "        & + \\frac1\\eta\n",
    "              \\sum_{m=1}^{d_1} \\lambda_m'\\bigl(\\delta_m - (Z_m - W_0'e_m)\\bigr)\n",
    "          + \\frac1{2\\eta}\n",
    "              \\sum_{m=1}^{d_1} \\bigl\\| \\delta_m - (Z_m - W_0'e_m) \\bigr\\|_2^2\n",
    "        \\,.\n",
    "\\end{align}\n",
    "Note the following expressions\n",
    "\\begin{align}\n",
    "    \\sum_{m=1}^{d_1} \\lambda_m'\\bigl(\\delta_m - (Z_m - W_0'e_m)\\bigr)\n",
    "        &= \\mathtt{tr}\\bigl((\\delta - (Z - W_0))\\Lambda'\\bigr) \\,,\n",
    "        \\\\\n",
    "    \\sum_{m=1}^{d_1} \\bigl\\| \\delta_m - (Z_m - W_0'e_m) \\bigr\\|_2^2\n",
    "        &= \\Bigl\\| \\delta - (Z - W_0) \\Bigr\\|_\\text{F}^2 \\,,\n",
    "        \\\\\n",
    "\\end{align}\n",
    "where $\\Lambda = \\sum_{m=1}^{d_1}e_m \\lambda_m'$ and $Z = \\sum_{m=1}^{d_1}e_m Z_m'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following subproblem ($\\mathtt{Sub}_0^\\text{QA}$):\n",
    "$$\\begin{aligned}\n",
    "    & \\underset{\\delta \\in \\mathbb{R}^{d_1\\times k}}{\\text{miminize}}\n",
    "      & & \\nabla F(W_0)' \\mathtt{vec}(\\delta)\n",
    "            + \\frac12 \\mathtt{vec}(\\delta)' \\nabla^2 F(W_0) \\mathtt{vec}(\\delta)\n",
    "          \\\\\n",
    "%     & & & + \\frac1\\eta\n",
    "%               \\mathtt{tr}\\bigl((\\delta - (Z - W_0))\\Lambda'\\bigr)\n",
    "%           + \\frac1{2\\eta}\n",
    "%               \\Bigl\\| \\delta - (Z - W_0) \\Bigr\\|_\\text{F}^2\n",
    "    & & & + \\frac1{2\\eta}\n",
    "              \\Bigl\\| \\delta + W_0 - Z + \\Lambda \\Bigr\\|_\\text{F}^2\n",
    "          - \\frac1{2\\eta} \\| \\Lambda \\|_\\text{F}^2\n",
    "            \\,.\n",
    "\\end{aligned}$$\n",
    "The first-order-conditions for this convex problem w.r.t. $\\mathtt{vec}(\\delta)$\n",
    "are\n",
    "$$ \\nabla F(W_0) + \\nabla^2 F(W_0) \\mathtt{vec}(\\delta)\n",
    "    + \\frac1\\eta\n",
    "        \\mathtt{vec}\\bigl( \\delta - (\\underbrace{Z - W_0 - \\Lambda}_{D}) \\bigr)\n",
    "    = 0 \\,. $$\n",
    "Since computing the inverse of the hessian is out of the quiestion, we use Conjugate\n",
    "Gradient method to solve for $\\delta$, because it queries the hessian\n",
    "only through matrix-vector priducts, which are efficicnetly computable.\n",
    "\n",
    "\n",
    "The map $\\mathtt{Sub}_0^\\text{QA}(D; \\eta)$ returns the $\\delta$ which satisfies\n",
    "$$\\Bigl( \\nabla^2 F(W_0) + \\frac1\\eta I\\Bigr)\\mathtt{vec}(\\delta)\n",
    "    = \\frac1\\eta \\mathtt{vec}\\bigl(D \\bigr) - \\nabla F(W_0) \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgimc.algorithm.admm import sub_0_cg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a more comprehensive solver, like `L-BFGS` we can tackle the original\n",
    "objective, instead of its Quadratic approximation.\n",
    "\n",
    "Consider the subproblem ($\\mathtt{Sub}_0^\\text{Orig}$):\n",
    "$$\\begin{aligned}\n",
    "    & \\underset{W \\in \\mathbb{R}^{d_1\\times k}}{\\text{miminize}}\n",
    "      & & F(W; H) + \\frac1{2\\eta}\n",
    "              \\Bigl\\|W - Z + \\Lambda \\Bigr\\|_\\text{F}^2\n",
    "          - \\frac1{2\\eta} \\| \\Lambda \\|_\\text{F}^2\n",
    "            \\,.\n",
    "\\end{aligned}$$\n",
    "The L-BFGS requires the gradient of the final objective:\n",
    "$$ \\nabla F(W)\n",
    "    + \\frac1\\eta\n",
    "        \\mathtt{vec}\\bigl( W - (Z - \\Lambda) \\bigr) \\,. $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgimc.algorithm.admm import sub_0_lbfgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of subproblems is represented by the following problem ($\\mathtt{Sub}_m$):\n",
    "$$\\begin{aligned}\n",
    "    & \\underset{Z_m \\in \\mathbb{R}^{k\\times 1}}{\\text{miminize}}\n",
    "      & & \\mu_m \\bigl\\| Z_m \\bigr\\|_1 + \\nu_m \\bigl\\| Z_m \\bigr\\|_2\n",
    "          + \\frac{\\kappa_m}2 \\bigl\\| Z_m \\bigr\\|_2^2\n",
    "          \\\\\n",
    "%     & & & + \\frac1\\eta \\lambda_m'\\bigl(\\delta_m - (Z_m - W_0'e_m)\\bigr)\n",
    "%           + \\frac1{2\\eta} \\bigl\\| \\delta_m - (Z_m - W_0'e_m) \\bigr\\|_2^2\n",
    "    & & & + \\frac1{2\\eta} \\bigl\\| (\\delta_m + W_0'e_m + \\lambda_m) - Z_m\\bigr\\|_2^2\n",
    "          - \\frac1{2\\eta} \\| \\lambda_m \\|_2^2\n",
    "            \\,.\n",
    "\\end{aligned}$$\n",
    "After a **lot of math** this problem admits a closed form solution:\n",
    "$$ Z_m\n",
    "    = \\frac1{1 + \\kappa_m \\eta}\n",
    "      \\biggl(1 - \\frac{\\nu_m \\eta}{\\|S(V_m; \\mu_m \\eta)\\|_2}\\biggr)_+\n",
    "        S(V_m; \\mu_m \\eta)\n",
    "    \\,, $$\n",
    "where $V_m = \\delta_m + W_0'e_m + \\lambda_m$ and \n",
    "$$ S(u; \\mu_m \\eta)\n",
    "    = \\Bigl(\\Bigl(1 - \\frac{\\mu_m \\eta}{\\lvert u_i \\rvert}\\Bigr)_+ u_i\\Bigr)_{i=1}^k\\,, $$\n",
    "is the **soft_thresholding** operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map $\\mathtt{Sub}_m(D; \\eta)$ returns $Z_m$ defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgimc.algorithm.admm import sub_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the QA-ADMM for $W$ around $W_0$ with $H$ fixed is the follwing\n",
    "iterative procedure:\n",
    "\\begin{align}\n",
    "    Z^{t+1}_m &= \\mathtt{Sub}_m(W^t_m + \\lambda^t_m) \\,,\\, m = 1\\ldots d_1 \\,,\\\\\n",
    "    W^{t+1} &= \\mathtt{Sub}_0(Z^{t+1} - W_0 - \\Lambda^t) + W_0  \\,,\\\\\n",
    "    % W^{t+1} &= \\mathtt{Sub}_0(Z^t - W_0 - \\Lambda^t) + W_0  \\,,\\\\\n",
    "    % Z^{t+1}_m &= \\mathtt{Sub}_m(W^{t+1}_m + \\lambda^t_m) \\,,\\, m = 1\\ldots d_1 \\,,\\\\\n",
    "    \\Lambda^{t+1} &= \\Lambda^t + (W^{t+1} - Z^{t+1})\\,,\\\\\n",
    "\\end{align}\n",
    "where $W^{t+1}_m$ is the $m$-th row of $W^{t+1}$, $Z^{t+1}_m$ is the\n",
    "$m$-th row of $Z^{t+1}$ and $\\lambda_m$ is the $m$-th row of $\\Lambda$.\n",
    "These iterations necessarily converge to a fixed point, which is the\n",
    "solution of the original optimisation problem. If stopped early, the\n",
    "current values of $W^t$ and $Z^t$ would be close to each other, however\n",
    "$Z^t$ would be sparse and $W^t$ -- dense.\n",
    "\n",
    "Note that we can also consider ADMM with a linear approximation of $F$\n",
    "w.r.t. $W$ at $W_0$, instead of the quadratic (LA-ADMM). This way the algorithm\n",
    "reduces to prox-gradient descent with step $\\eta$. Although it does not utilize\n",
    "the second order infromation, it can be fused with Nesterov's Accelerated\n",
    "gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgimc.algorithm import admm_step\n",
    "\n",
    "def step_qaadmm(X, W, Y, H, R, C, eta, method=\"l-bfgs\", sparse=True,\n",
    "                n_iterations=50, rtol=1e-5, atol=1e-8, n_threads=4):\n",
    "\n",
    "    b_hessian = method in (\"cg\",)\n",
    "\n",
    "    Obj = QAObjectiveLoss(X, W, Y, H, R, hessian=b_hessian,\n",
    "                          n_threads=n_threads)\n",
    "\n",
    "    return admm_step(Obj, W, C, eta, sparse=sparse, method=method,\n",
    "                     n_iterations=n_iterations, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgimc.algorithm.decoupled import step as decoupled_step\n",
    "\n",
    "def step_decoupled(X, W, Y, H, R, C, eta,\n",
    "                   rtol=1e-5, atol=1e-8, n_threads=4):\n",
    "\n",
    "    Obj = QAObjectiveLoss(X, W, Y, H, R, hessian=False,\n",
    "                          n_threads=n_threads)\n",
    "\n",
    "    return decoupled_step(Obj, W, C, eta, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ad-hoc procedure. No guarantees for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def step_adhoc(X, W, Y, H, R, C, eta, rtol=1e-5, atol=1e-8, n_threads=4):\n",
    "#     Obj = QAObjectiveLoss(X, W, Y, H, R, hessian=True,\n",
    "#                           n_threads=n_threads)\n",
    "\n",
    "#     delta = sub_0_cg(np.zeros_like(W), Obj, eta=eta, tol=1e-8)\n",
    "#     return sub_m(delta + W, *C, eta=eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def QA_argmin(D, Obj, tol=1e-8):\n",
    "\n",
    "#     # set up the CG arguments\n",
    "#     x = D.reshape(-1).copy()\n",
    "#     b = - Obj.grad().reshape(-1)\n",
    "#     Ax = lambda x: Obj.hess_v(x.reshape(D.shape)).reshape(-1)\n",
    "\n",
    "#     n_iter = simple_cg(Ax, b, x, tol=tol)\n",
    "#     return x.reshape(D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus Sparse Group IMC via QA-ADMM is the follwing iterative procedure:\n",
    "* $W^{t+1} = \\mathtt{ADMM}\\bigl(W^t; H^t\\bigr)$,\n",
    "* $H^{t+1} = \\mathtt{ADMM}\\bigl(H^t; W^{t+1}\\bigr)$,\n",
    "\n",
    "until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgimc import imc_descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss information: value and regularization on the train data and value of the full matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgimc.util import performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_fn = step_qaadmm\n",
    "# step_fn = step_decoupled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bigl(C_\\mathtt{lasso}, C_\\mathtt{group}, C_\\mathtt{ridge}\\bigr) = C \\,.$$\n",
    "It seems that it must hold $C_\\mathtt{lasso} > C_\\mathtt{group}$ so that\n",
    "individual sparsity preceeds group sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROBLEM == \"classification\":\n",
    "    C = 1e0, 1e-1, 1e-3\n",
    "    eta = 1e0\n",
    "else:\n",
    "    # C = 2e-5, 2e-3, 0\n",
    "    C = 2e-3, 2e-4, 1e-4  # 1e-2\n",
    "    eta = 1e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if step_fn == step_decoupled:\n",
    "    eta = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10  # n_rank\n",
    "# K = n_rank\n",
    "\n",
    "W_0 = random_state.normal(size=(X.shape[1], K))\n",
    "H_0 = random_state.normal(size=(Y.shape[1], K))\n",
    "\n",
    "# W_0 = W_ideal.copy() # + random_state.normal(scale=0.1, size=(X.shape[1], K))\n",
    "# H_0 = H_ideal.copy() # + random_state.normal(scale=0.1, size=(Y.shape[1], K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in this experiment the ideal solution is a unit matrix stacked atop a zero martix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_WH(W_ideal, H_ideal)\n",
    "\n",
    "loss_arr, exp_type, norm_type = performance(X, W_ideal,\n",
    "                                            Y, H_ideal,\n",
    "                                            R, C, R_full,\n",
    "                                            QAObjectiveLoss)\n",
    "\n",
    "print(\"The loss on the initial guess is:\")\n",
    "print(\"%.3e + %.3e -- partial matrix\" % (loss_arr[0, -1], loss_arr[1, -1]))\n",
    "print(\"%.3e -- full matrix\" % loss_arr[3, -1])\n",
    "print(\"score %.4f\" % loss_arr[2, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.dot(W_ideal, H_ideal.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial guess is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_WH(W_0, H_0)\n",
    "\n",
    "loss_arr, exp_type, norm_type = performance(X, W_0,\n",
    "                                            Y, H_0,\n",
    "                                            R, C, R_full,\n",
    "                                            QAObjectiveLoss)\n",
    "\n",
    "print(\"The loss on the initial guess is:\")\n",
    "print(\"%.3e + %.3e -- partial matrix\" % (loss_arr[0, -1], loss_arr[1, -1]))\n",
    "print(\"%.3e -- full matrix\" % loss_arr[3, -1])\n",
    "print(\"score %.4f\" % loss_arr[2, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.dot(W_0, H_0.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H = W_0.copy(), H_0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W, H = imc_descent(\n",
    "    X, W, Y, H, R, C, step_fn, eta=eta,\n",
    "    n_iterations=12500, rtol=1e-5, atol=1e-8,\n",
    "    verbose=True, return_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_WH(abs(W[..., -1]), abs(H[..., -1]))\n",
    "\n",
    "loss_arr, exp_type, norm_type = performance(X, W, Y, H, R, C, R_full,\n",
    "                                            QAObjectiveLoss)\n",
    "\n",
    "print(\"The loss on the final estimates is:\")\n",
    "print(\"%.3e + %.3e -- partial matrix\" % (loss_arr[0, -1], loss_arr[1, -1]))\n",
    "print(\"%.3e -- full matrix\" % loss_arr[3, -1])\n",
    "print(\"score %.4f\" % loss_arr[2, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.dot(W[..., -1], H[..., -1].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, title=\"Elementwise loss value\")\n",
    "R_hat = np.dot(np.dot(X, W[..., -1]), np.dot(Y, H[..., -1]).T)\n",
    "ax.imshow(QAObjectiveLoss.v_func(R_hat, R_full), cmap=plt.cm.hot, origin=\"upper\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(np.array([\"#\", \".\"])[np.isclose(W[..., -1], 0)*1]).replace(\"' '\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(np.array([\"#\", \".\"])[np.isclose(H[..., -1], 0)*1]).replace(\"' '\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(W[..., -1], 2, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(H[..., -1], 2, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(~ np.isclose(np.dot(W[..., -1], H[..., -1].T),\n",
    "                       np.dot(W_ideal, H_ideal.T)),\n",
    "          cmap=plt.cm.binary_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(abs(np.dot(W[..., -1], H[..., -1].T)).reshape(-1), bins=20) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(abs(R_hat).reshape(-1), bins=200) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d', xlabel=\"col\", ylabel=\"row\")\n",
    "\n",
    "ZZ = QAObjectiveLoss.v_func(R_hat, R_full)\n",
    "mesh_ = np.meshgrid(*[np.linspace(0, 1, num=n) for n in ZZ.shape[::-1]])\n",
    "\n",
    "surf = ax.plot_surface(*mesh_, ZZ, alpha=0.5, lw=0, antialiased=True,\n",
    "                       cmap=plt.cm.coolwarm)\n",
    "\n",
    "fig.colorbar(surf, shrink=0.5, aspect=10)\n",
    "\n",
    "ax.view_init(37, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ[~mask].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ[mask].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d', xlabel=\"col\", ylabel=\"row\")\n",
    "\n",
    "ZZ = np.dot(W[..., -1], H[..., -1].T)\n",
    "mesh_ = np.meshgrid(*[np.linspace(0, 1, num=n) for n in ZZ.shape[::-1]])\n",
    "\n",
    "surf = ax.plot_surface(*mesh_, ZZ, alpha=0.5, lw=0, antialiased=True,\n",
    "                       cmap=plt.cm.coolwarm)\n",
    "\n",
    "fig.colorbar(surf, shrink=0.5, aspect=10)\n",
    "\n",
    "ax.view_init(37, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"\"\"STOP!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-loss $l(p, t) = - t \\log \\sigma(p) - (1-t) \\log (1-\\sigma(p))$.\n",
    "\n",
    "\\begin{align}\n",
    "    \\sigma(x)\n",
    "        &= \\frac1{1+e^{-x}}\n",
    "        \\,, \\\\\n",
    "    \\sigma'(x)\n",
    "        &= -\\frac{- e^{-x}}{(1+e^{-x})^2}\n",
    "        = \\frac{e^{-x}}{1+e^{-x}} \\frac1{1+e^{-x}}\n",
    "        = (1-\\sigma(x))\\sigma(x)\n",
    "        \\,, \\\\\n",
    "    l_p'(p, t)\n",
    "        &= \\frac{1-t}{1-\\sigma(p)}\\sigma'(p) - \\frac{t}{\\sigma(p)}\\sigma'(p)\n",
    "        = \\frac{\\sigma(p) - t}{(1-\\sigma(p))\\sigma(p)}\\sigma'(p)\n",
    "        = \\sigma(p) - t\n",
    "        \\,, \\\\\n",
    "      l_p''(p, t)\n",
    "        &= \\sigma'(p)\n",
    "        = (1-\\sigma(p))\\sigma(p)\n",
    "        \\,.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAObjectiveLogLoss_01(QAObjective):\n",
    "    def __init__(self, X, W, Y, H, R, n_threads=4):\n",
    "        super(QAObjectiveLogLoss_01, self).__init__(\n",
    "            X, W, Y, H, R, n_threads=n_threads)\n",
    "\n",
    "    @staticmethod\n",
    "    def v_func(logit, target):\n",
    "        out = np.log1p(np.exp(- abs(logit))) - logit * target\n",
    "        out += np.maximum(logit, 0)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def g_func(logit, target):\n",
    "        return sigmoid(logit) - target\n",
    "\n",
    "    @staticmethod\n",
    "    def h_func(logit, target):\n",
    "        prob = sigmoid(logit)\n",
    "        return prob * (1 - prob)\n",
    "\n",
    "    @staticmethod\n",
    "    def score(logit, target):\n",
    "        predict = (sigmoid(logit) > 0.5) * 1\n",
    "        return (predict != target).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XF, ZF = X.copy(\"F\"), np.dot(Y, H).copy(\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = QAObjectiveLoss(X, W, Y, H, R, n_threads=4)\n",
    "Z = np.zeros_like(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "obj.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "obj.hess_v(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = op_d(R, XF, ZF, W.copy(\"F\"), 1)\n",
    "r2 = op_d(R, XF, ZF, W.copy(\"F\"), 4)\n",
    "\n",
    "assert np.allclose(r1, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "op_d(R, XF, ZF, W.copy(\"F\"), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "op_d(R, XF, ZF, W.copy(\"F\"), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "op_d(R, XF, ZF, W.copy(\"F\"), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "op_d(R, XF, ZF, W.copy(\"F\"), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = op_s(R, XF, ZF, R.data, 1)\n",
    "r2 = op_s(R, XF, ZF, R.data, 4)\n",
    "\n",
    "assert np.allclose(r1, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "r1 = op_s(R, XF, ZF, R.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "r2 = op_s(R, XF, ZF, R.data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "r2 = op_s(R, XF, ZF, R.data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "r2 = op_s(R, XF, ZF, R.data, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"\"\"STOP!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XF, ZF = X.copy(\"F\"), np.dot(Y, H).copy(\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = QAObjectiveLoss(X, W, Y, H, R, n_threads=-1)\n",
    "Z = np.zeros_like(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "obj.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "obj.hess_v(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "obj.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "obj.hess_v(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "op_d(R, XF, ZF, W.copy(\"F\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = op_s(R, XF, ZF, R.data, 1)\n",
    "r2 = op_s(R, XF, ZF, R.data, -7)\n",
    "\n",
    "assert np.allclose(r1, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "r1 = op_s(R, XF, ZF, R.data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "r2 = op_s(R, XF, ZF, R.data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"\"\"STOP!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_lasso, C_group, C_ridge = C\n",
    "Obj = QAObjectiveLoss(X, W, Y, H, R)  # , hessian=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LL, WW, ZZ = np.zeros_like(W), W.copy(), W.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WW_old = WW.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ZZ = sub_m(WW + LL, C_lasso, C_group, C_ridge, eta=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ = sub_m(WW + LL, C_lasso, C_group, C_ridge, eta=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "WW = solver_sub_0(ZZ - LL, Obj, x0=WW_old, eta=eta,\n",
    "                  tol=1e-8, method=\"l-bfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "WW = W + sub_0((ZZ - W) - LL, Obj, eta=eta,\n",
    "               tol=1e-8, linearize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "WW = solver_sub_0(ZZ - LL, Obj, x0=W.copy(), eta=eta,\n",
    "                  tol=1e-8, method=\"ncg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "check_grad(f_value, f_prime, W.reshape(-1), Obj, W, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
