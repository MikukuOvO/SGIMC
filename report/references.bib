@article{boydetal2011,
    author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
    title = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},
    journal = {Foundations and Trends in Machine Learning},
    issue_date = {January 2011},
    volume = {3},
    number = {1},
    month = jan,
    year = {2011},
    issn = {1935-8237},
    pages = {1--122},
    numpages = {122},
    url = {http://dx.doi.org/10.1561/2200000016},
    doi = {10.1561/2200000016},
    acmid = {2185816},
    publisher = {Now Publishers Inc.},
    address = {Hanover, MA, USA},
}


@article{parikhboyd2014,
    author = {Parikh, Neal and Boyd, Stephen},
    title = {Proximal Algorithms},
    journal = {Foundations and Trends in Optimization},
    issue_date = {January 2014},
    volume = {1},
    number = {3},
    month = jan,
    year = {2014},
    issn = {2167-3888},
    pages = {127--239},
    numpages = {113},
    url = {http://dx.doi.org/10.1561/2400000003},
    doi = {10.1561/2400000003},
    acmid = {2693613},
    publisher = {Now Publishers Inc.},
    address = {Hanover, MA, USA},
}


@article{polsonetal2015,
    author = {{Polson}, Nicholas G. and {Scott}, James G. and {Willard}, Brandon T.},
    title = {Proximal Algorithms in Statistics and Machine Learning},
    journal = {ArXiv e-prints},
    archivePrefix = "arXiv",
    eprint = {1502.03175},
    primaryClass = "stat.ML",
    keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Methodology},
    year = {2015},
    month = {feb},
    adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150203175P},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{simonetal2013,
    author={Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
    title={A Sparse-Group Lasso},
    journal={Journal of Computational and Graphical Statistics},
    volume={22},
    number={2},
    pages={231-245},
    year={2013},
    doi={10.1080/10618600.2012.681250},
    URL={http://dx.doi.org/10.1080/10618600.2012.681250},
    eprint={http://dx.doi.org/10.1080/10618600.2012.681250},
    abstract={For high-dimensional supervised learning problems, often using problem-specific assumptions can lead to greater accuracy. For problems with grouped covariates, which are believed to have sparse effects both on a group and within group level, we introduce a regularized model for linear regression with ℓ1 and ℓ2 penalties. We discuss the sparsity and other regularization properties of the optimal fit for this model, and show that it has the desired effect of group-wise and within group sparsity. We propose an algorithm to fit the model via accelerated generalized gradient descent, and extend this model and algorithm to convex loss functions. We also demonstrate the efficacy of our model and the efficiency of our algorithm on simulated data. This article has online supplementary material. }
}


@inproceedings{chiangetal2015,
    author = {Chiang, Kai-Yang and Hsieh, Cho-Jui and Dhillon, Inderjit S.},
    title = {Matrix Completion with Noisy Side Information},
    booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
    editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
    series = {NIPS'15},
    year = {2015},
    location = {Montreal, Canada},
    pages = {3447--3455},
    numpages = {9},
    url = {http://dl.acm.org/citation.cfm?id=2969442.2969624},
    acmid = {2969624},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/5940-matrix-completion-with-noisy-side-information.pdf},
    address = {Cambridge, MA, USA}
}


@inproceedings{yuetal2014,
    author = {Yu, Hsiang-Fu and Jain, Prateek and Kar, Purushottam and Dhillon, Inderjit S.},
    title = {Large-scale Multi-label Learning with Missing Labels},
    booktitle = {International Conference on Machine Learning (ICML)},
    page = {593—-601},
    volume = {32},
    issue = {1},
    year = {2014},
    month = {jun},
    abstract = {The multi-label classification problem has generated significant interest in recent years. However, existing approaches do not adequately address two key challenges: (a) scaling up to problems with a large number (say millions) of labels, and (b) handling data with missing labels. In this paper, we directly address both these problems by studying the multi-label problem in a generic empirical risk minimization (ERM) framework. Our framework, despite being simple, is surprisingly able to encompass several recent label-compression based methods which can be derived as special cases of our method. To optimize the ERM problem, we develop techniques that exploit the structure of specific loss functions – such as the squared loss function – to obtain efficient algorithms. We further show that our learning framework admits excess risk bounds even in the presence of missing labels. Our bounds are tight and demonstrate better generalization performance for low-rank promoting trace-norm regularization when compared to (rank insensitive) Frobenius norm regularization. Finally, we present extensive empirical results on a variety of benchmark datasets and show that our methods perform significantly better than existing label compression based methods and can scale up to very large datasets such as a Wikipedia dataset that has more than 200,000 labels.}
}
